---
title: "Testing various ML models in heart disease prediction"
author: "P. Boadiwaa Mensah"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(reticulate)
```

```{python}
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.offline as py
import plotly.graph_objects as go
import plotly.express as px
import plotly.io as pio 
import lightgbm as lgb
import plotly.figure_factory as ff

from colorama import Fore, Back, Style  #for formatting the output font of printed texts
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score,recall_score
from mlxtend.plotting import plot_confusion_matrix
from plotly.offline import plot, iplot, init_notebook_mode
import plotly.graph_objs as go
from plotly.subplots import make_subplots
from statsmodels.formula.api import ols
import plotly.graph_objs as gobj
 
from imblearn.over_sampling import SMOTE
import xgboost
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

``` 

### Overview of Dataset

The dataset was obtained from Kaggle. It had 299 observations and 13 variables. the outcome variable 'DEATH_EVENT' indicates whether a patient died of heart failure or not based on 11 other predictors. The variable names are shown below:

NB: *The 12th variable 'time' indicated the time from the start of the study after which the study was terminated. This,presumably,could be either because the subject was declared healthy, or dropped out of the study for various reasons, or died from heart failure. To avoid target leakage, since that time would not be available in real world instances when the resultant model is being used to predict the outcome of a new case, the 'time' variable would not be used as a feature to train the model.*

```{python}
data = pd.read_csv("C:/Users/pauli/Downloads/heart_failure_clinical_records_dataset.csv")
print(data.columns)
```

### Brief Exploratory Data Analysis

We can do a quick overview of the only two demographic variables from the dataset: age and sex. from the output below, we realize that the age range of the respondents is 40 to 95 years with a median age of 60 years and an average age of approximately 60 years.
```{python}
print(data['age'].describe())
```

```{python}
hist_data = [data["age"].values]
group_labels = ['age']

agedist = ff.create_distplot(hist_data, group_labels,show_rug=False, colors=['#37AA9C'])
agedist.update_layout(title_text = 'Age Distribution of Dataset')
#py.plot(agedist, filename = "agedist.html", auto_open=False)

```

```{python}
df = data
df["Mortality"] =  np.where(df["DEATH_EVENT"] == 0, "No", "Yes")
outcome = px.histogram(df, x= "Mortality", text_auto=True)
outcome.update_layout(bargap=0.5, title_text= "Count of Outcomes")

```


```{python}
gendist = df['sex'].value_counts().to_frame().reset_index(level=0)
gendist.replace([1,0],["Male", "Female"], inplace = True)
genfig = px.pie(gendist, values = 'sex', names='index', title  = "Gender Distribution of Dataset" )
genfig.show()
```

### Pre-selection of Features and Feature Engineering

We are closer to our goal of comparing the performance of various ML models on the dataset. Features here are pre-selected based on domain knowledge. Next, some feature engineering with Synthetic Minority Oversampling Technique (SMOTE). SMOTE utilizes a k-nearest neighbour algorithm helps to overcome the overfitting problem posed by random oversampling.In our dataset, the proportion of "No" examples for our outcome variable is much higher than "Yes" examples. Thus, there is the danger of our ML algorithms being biased if trained on this data as they would have way more "No" examples to learn from. I chose SMOTE instead of Random undersampling of the majority class because I want to preserve the data and not eliminate any examples since I do not have much training data to begin with!

```{python}
Features = ['age','anaemia','creatinine_phosphokinase', 'diabetes',
            'ejection_fraction', 'serum_sodium', 'high_blood_pressure','smoking']

x = data[Features]
y = data["DEATH_EVENT"]
x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.2, random_state=2)

#Visualization of training data before sampling technique
traindata = px.histogram(y_train, x= "DEATH_EVENT", text_auto=True)
traindata.update_layout(bargap=0.5, title_text= "Count of Outcomes for Training Dataset")

```

```{python}
smote = SMOTE(sampling_strategy = 'minority')
x_train_SMOTE, y_train_SMOTE = smote.fit_resample(x_train,y_train)

#Visualization of training data after sampling technique
traindata_sampled = px.histogram(y_train_SMOTE, x= "DEATH_EVENT", text_auto=True)
traindata_sampled.update_layout(bargap=0.5, title_text= "Count of Outcomes for Training Dataset after Sampling")

```

### Feature Selection

First, we identify features with low variance since they would not help the model much in finding patterns and de-select them. We will also check if there is multicollinearity amongst any of the features and we de-select one per pair.

```{python}
var = VarianceThreshold(threshold = 0.05) # arbritary percentage of 5% for the minimum variance to be allowed. Thus features with a minimum of 95% similarity of values would be dropped.
var.fit(x) 
var.get_support()

```
From the results, per our threshold criteria, all the features have high variance (< 95% similarity amongst values).

```{python}
cor = x.corr() 
cor
sns.heatmap(cor)
``` 
There is no collinearity amongst the variables.



